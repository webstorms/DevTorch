{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe48dc5a-ef97-461a-82d5-69876781be0c",
   "metadata": {},
   "source": [
    "### Goal\n",
    "This code example shows you how to fruther customize the devtorch trainer - given you a little more more power over the training process to potentially accelerate training and make training more stable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e4f0ab3-b30f-4892-854b-34647b843f4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import devtorch\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb99e371-f95e-42e1-87d4-70809708a192",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ANNClassifier(devtorch.DevModel):\n",
    "    \n",
    "    def __init__(self, n_in, n_hidden, n_out):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(n_in, n_hidden, bias=False)\n",
    "        self.layer2 = nn.Linear(n_hidden, n_out, bias=False)\n",
    "        self.init_weight(self.layer1.weight, \"glorot_uniform\")\n",
    "        self.init_weight(self.layer2.weight, \"glorot_uniform\")\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.leaky_relu(self.layer1(x.flatten(1, 3)))\n",
    "        return F.leaky_relu(self.layer2(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a460107-2076-4f7e-9356-1904d6563162",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:trainer:Completed epoch 0 with loss 149.45636756252497 in 7.6585s\n",
      "INFO:trainer:Completed epoch 1 with loss 39.71987604862079 in 7.6597s\n",
      "INFO:trainer:Completed epoch 2 with loss 22.427124134730548 in 7.6438s\n",
      "INFO:trainer:Completed epoch 3 with loss 14.296442218270386 in 7.6427s\n",
      "INFO:trainer:Completed epoch 4 with loss 9.480451079107297 in 7.6288s\n",
      "INFO:trainer:Completed epoch 5 with loss 5.96110432043497 in 7.6378s\n",
      "INFO:trainer:Completed epoch 6 with loss 3.871650189132197 in 7.6436s\n",
      "INFO:trainer:Completed epoch 7 with loss 3.1468983609738643 in 7.6309s\n",
      "INFO:trainer:Completed epoch 8 with loss 2.915643396354426 in 7.6351s\n",
      "INFO:trainer:Completed epoch 9 with loss 1.5679306890942826 in 7.6324s\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
    "train_dataset = datasets.MNIST(\"../../data\", train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST(\"../../data\", train=False, download=True, transform=transform)\n",
    "\n",
    "def loss(output, target, model):\n",
    "    return F.cross_entropy(output, target.long())\n",
    "\n",
    "\n",
    "# Full on trainer \n",
    "model = ANNClassifier(784, 4000, 10)\n",
    "n_epochs=100\n",
    "batch_size=128\n",
    "lr=0.001\n",
    "optimizer_func=torch.optim.Adam  # Can swap out for any other torch optimizer (see https://pytorch.org/docs/stable/optim.html)\n",
    "scheduler_func=torch.optim.lr_scheduler.ExponentialLR  # Can specify a LR scheduler (see https://pytorch.org/docs/stable/optim.html)\n",
    "device=\"cuda\"  # Make GPU go brrrr or switch out for \"cpu\"\n",
    "dtype=torch.float  # Changing the dtype to half precision (torch.half) can speedup training\n",
    "\n",
    "# Grad clipping can stabilize training by preventing gradients from blowing up\n",
    "# A good online-tutorial should cover these different types (https://machinelearningmastery.com/how-to-avoid-exploding-gradients-in-neural-networks-with-gradient-clipping/)\n",
    "# Options are: \"GRAD_VALUE_CLIP_PRE\", \"GRAD_VALUE_CLIP_POST\" or \"GRAD_NORM_CLIP\"\n",
    "grad_clip_type=\"GRAD_VALUE_CLIP_POST\"\n",
    "grad_clip_value=0.05  # Usually determined by trial-and-error\n",
    "\n",
    "save_type=\"SAVE_DICT\"  # Preferred default, you can also save as \"SAVE_OBJECT\"\n",
    "# The name of the folder where all model params, logs and hyperparams are stored\n",
    "# You will also need to define a root and call trainer.train(save=True)\n",
    "id=\"my_awesome_model\" \n",
    "\n",
    "# You can pass arguments to the torch optimizer, scheduler and data loader using this dicts\n",
    "optimizer_kwargs={}\n",
    "scheduler_kwargs={\"gamma\": 0.9}\n",
    "loader_kwargs={}\n",
    "\n",
    "trainer = devtorch.get_trainer(loss, \n",
    "                               model=model, \n",
    "                               train_dataset=train_dataset, \n",
    "                               n_epochs=10, \n",
    "                               batch_size=128, \n",
    "                               lr=0.001, \n",
    "                               optimizer_func=optimizer_func,\n",
    "                               scheduler_func=scheduler_func,\n",
    "                               device=device,\n",
    "                               dtype=dtype,\n",
    "                               grad_clip_type=grad_clip_type,\n",
    "                               grad_clip_value=grad_clip_value,\n",
    "                               scheduler_kwargs=scheduler_kwargs)\n",
    "                               \n",
    "trainer.train(save=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12bb891b-07a0-4894-afff-790d7762a2c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.984499990940094\n"
     ]
    }
   ],
   "source": [
    "def eval_metric(output, target):\n",
    "    return (torch.max(output, 1)[1] == target).sum().cpu().item()\n",
    "\n",
    "scores = devtorch.compute_metric(model, test_dataset, eval_metric, batch_size=256)\n",
    "print(f\"Accuracy = {torch.Tensor(scores).sum()/len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673d7712-3c6e-421f-b980-a076a0d27079",
   "metadata": {},
   "source": [
    "**Exercise**: You could try fruther tweak the training arguments to improve the test score."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:devtorch] *",
   "language": "python",
   "name": "conda-env-devtorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
